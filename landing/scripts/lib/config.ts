import type { MarketplaceDef, MarketplaceName } from "./types";

export const MARKETPLACES: Record<MarketplaceName, MarketplaceDef> = {
  npm: {
    name: "npm",
    url: "https://www.npmjs.com",
    description: "Node.js package registry",
  },
  PyPI: {
    name: "PyPI",
    url: "https://pypi.org",
    description: "Python Package Index",
  },
  Smithery: {
    name: "Smithery",
    url: "https://smithery.ai",
    description: "MCP server registry",
  },
  Glama: {
    name: "Glama",
    url: "https://glama.ai",
    description: "MCP server directory",
  },
  "MCP Registry": {
    name: "MCP Registry",
    url: "https://registry.modelcontextprotocol.io",
    description: "Official MCP server registry",
  },
  SkillsMP: {
    name: "SkillsMP",
    url: "https://skillsmp.com",
    description: "Agent skills marketplace",
  },
  PulseMCP: {
    name: "PulseMCP",
    url: "https://www.pulsemcp.com",
    description: "MCP server directory with enriched metadata",
  },
};

export const NPM_SEARCH_QUERIES = [
  "mcp server",
  "ai agent",
  "claude tool",
  "langchain",
  "model context protocol",
  "llm tool",
  "openai plugin",
  "ai assistant tool",
];

export const PYPI_PACKAGES = [
  "mcp",
  "langchain",
  "langchain-core",
  "langchain-community",
  "langchain-openai",
  "llama-index",
  "llama-index-core",
  "openai",
  "anthropic",
  "crewai",
  "autogen",
  "semantic-kernel",
  "guidance",
  "dspy-ai",
  "promptflow",
  "haystack-ai",
  "chromadb",
  "pinecone-client",
  "weaviate-client",
  "qdrant-client",
  "marvin",
  "instructor",
  "guardrails-ai",
  "nemoguardrails",
  "litellm",
  "vllm",
  "transformers",
  "sentence-transformers",
  "tiktoken",
  "tokenizers",
  "pydantic-ai",
  "mirascope",
  "magentic",
  "lmql",
  "outlines",
  "txtai",
  "embedchain",
  "memgpt",
  "letta",
  "agentops",
  "langfuse",
  "langsmith",
  "phoenix-ai",
  "trulens-eval",
  "ragas",
  "deepeval",
  "prompttools",
  "giskard",
  "whylogs",
  "evidently",
  "cleanlab",
  "snorkel-ai",
  "label-studio",
  "prodigy",
  "spacy-llm",
  "fastchat",
  "text-generation-inference",
  "ctransformers",
  "gpt4all",
  "pyllamacpp",
  "llama-cpp-python",
  "exllamav2",
  "auto-gptq",
  "bitsandbytes",
  "peft",
  "trl",
  "axolotl",
  "unsloth",
  "wandb",
  "mlflow",
  "bentoml",
  "ray",
  "modal",
  "gradio",
  "streamlit",
  "chainlit",
  "panel",
  "mesop",
  "fastapi",
  "flask",
  "django-ai",
  "celery",
  "prefect",
  "airflow",
  "dagster",
  "meltano",
  "dbt-core",
  "great-expectations",
  "pandera",
  "pydantic",
  "attrs",
  "cattrs",
  "msgspec",
  "orjson",
  "httpx",
  "aiohttp",
  "websockets",
  "mcp-server-sqlite",
  "mcp-server-git",
  "mcp-server-filesystem",
  "mcp-server-fetch",
];

export const GLAMA_CURATED_REPOS = [
  "anthropics/anthropic-tools",
  "modelcontextprotocol/servers",
  "langchain-ai/langchain",
  "run-llama/llama_index",
];

export const RATE_LIMITS: Record<MarketplaceName, { requestsPerSecond: number; maxPackages: number }> = {
  npm: { requestsPerSecond: 50, maxPackages: 200 },
  PyPI: { requestsPerSecond: 10, maxPackages: 100 },
  Smithery: { requestsPerSecond: 5, maxPackages: 150 },
  Glama: { requestsPerSecond: 3, maxPackages: 100 },
  "MCP Registry": { requestsPerSecond: 10, maxPackages: 500 },
  SkillsMP: { requestsPerSecond: 5, maxPackages: 200 },
  PulseMCP: { requestsPerSecond: 3, maxPackages: 300 },
};

export const SCANNER_TIMEOUT_MS = 120_000;
export const SCAN_STALENESS_HOURS = 24;
export const USER_AGENT = "i-see-u-scanner/0.1.0 (https://github.com/LiamCYD/icu)";
